<html lang="en-US">
  <head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">

<!-- Begin Jekyll SEO tag v2.5.0 -->
<title>Workshop on Machine Learning and User Decision Making | ml-udm.github.io</title>
<meta name="generator" content="Jekyll v3.7.4" />
<meta property="og:title" content="Workshop on Machine Learning and User Decision Making" />
<meta property="og:locale" content="en_US" />
<link rel="canonical" href="https://ml-udm.github.io/" />
<meta property="og:url" content="https://ml-udm.github.io/" />
<meta property="og:site_name" content="ml-udm.github.io" />
<script type="application/ld+json">
{"@type":"WebSite","url":"https://ml-udm.github.io/","headline":"Workshop on Machine Learning and User Decision Making","name":"ml-udm.github.io","@context":"http://schema.org"}</script>
<!-- End Jekyll SEO tag -->

    <link rel="stylesheet" href="/assets/css/style.css?v=1fd887da03f36fc680648dc1ab81fd840d673d54">
  </head>

<body>

<div class="container-lg px-3 my-5 markdown-body">

<h2>Title:</h2> 
  
Can RL learn classic optimization Algorithms?

<h2>Speaker:</h2> 
  
Aranyak Mehta, Google Research
  
<h2>Abstract:</h2>

<p>
We ask whether reinforcement learning can find theoretically optimal algorithms for online optimization problems, and introduce a novel learning framework in this setting. By "algorithms", we mean uniform "pen-and-paper" algorithms, which work for any input length, and have worst-case guarantees. To answer this question, we introduce a number of key ideas from traditional algorithms and complexity theory. Specifically, we introduce the concept of adversarial distributions (universal and high-entropy training sets), which are distributions that encourage the learner to find algorithms that work well in the worst case. We present results for three different representative problems --  the AdWords problem (aka online Budgeted Allocation), the online Knapsack problem, and the Secretary problem. Our results indicate that the models learn behaviors that are consistent with the well-known optimal algorithms derived using the online primal-dual framework.
</p>
<p>
Based on the paper "A new dog learns old tricks: RL finds classic optimization algorithms". Joint work with  Weiwei Kong, Christopher Liaw, and D. Sivakumar (ICLR 2019)
</p>

<h2></h2>  

<p>
Back to the <a href="https://ml-udm.github.io">workshop page</a>  
</p>
  
</div>
  
</body>  
  
</html>
