<html lang="en-US">
  <head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">

<!-- Begin Jekyll SEO tag v2.5.0 -->
<title>Workshop on Machine Learning and User Decision Making | ml-udm.github.io</title>
<meta name="generator" content="Jekyll v3.7.4" />
<meta property="og:title" content="Workshop on Machine Learning and User Decision Making" />
<meta property="og:locale" content="en_US" />
<link rel="canonical" href="https://ml-udm.github.io/" />
<meta property="og:url" content="https://ml-udm.github.io/" />
<meta property="og:site_name" content="ml-udm.github.io" />
<script type="application/ld+json">
{"@type":"WebSite","url":"https://ml-udm.github.io/","headline":"Workshop on Machine Learning and User Decision Making","name":"ml-udm.github.io","@context":"http://schema.org"}</script>
<!-- End Jekyll SEO tag -->

    <link rel="stylesheet" href="/assets/css/style.css?v=1fd887da03f36fc680648dc1ab81fd840d673d54">
  </head>

<body>

<div class="container-lg px-3 my-5 markdown-body">

<h2>Title:</h2> 
  
Decision theoretic approach for improved training of Generative Adversarial Networks

<h2>Speaker:</h2> 
  
<a href="https://www.engr.washington.edu/facresearch/newfaculty/2018/SewoongOh">Sewoong Oh</a>, University of Washington
  
<h2>Abstract:</h2>

<p>
We bring the tools from Blackwell’s seminal result on comparing two stochastic experiments from 1953, to shine a new light on a modern application of great interest: Generative Adversarial Networks (GAN). Binary hypothesis testing is at the center of training GANs, where a trained neural network (called a critic) determines whether a given sample is from the real data or the generated (fake) data. By jointly training the generator and the critic, the hope is that eventually the trained generator will generate realistic samples. One of the major challenges in GAN is known as “mode collapse”; the lack of diversity in the samples generated by thus trained generators. We propose a new training framework, where the critic is fed with multiple samples jointly (which we call packing), as opposed to each sample separately as done in standard GAN training. With this simple but fundamental departure from standard GANs, experimental results show that the diversity of the generated samples improve significantly. We analyze this practical gain by first providing a formal mathematical definition of mode collapse and making a fundamental connection between the idea of packing and the intensity of mode collapse. Precisely, we show that the packed critic naturally penalizes mode collapse, thus encouraging generators with less mode collapse. The analyses critically rely on operational interpretation of hypothesis testing and corresponding data processing inequalities, which lead to sharp analyses with simple proofs.
</p>

<h2></h2>  

<p>
Back to the <a href="https://ml-udm.github.io">workshop page</a>  
</p>
  
</div>
  
</body>  
  
</html>
