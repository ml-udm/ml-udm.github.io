<html lang="en-US">
  <head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">

<!-- Begin Jekyll SEO tag v2.5.0 -->
<title>Workshop on Machine Learning and User Decision Making | ml-udm.github.io</title>
<meta name="generator" content="Jekyll v3.7.4" />
<meta property="og:title" content="Workshop on Machine Learning and User Decision Making" />
<meta property="og:locale" content="en_US" />
<link rel="canonical" href="https://ml-udm.github.io/" />
<meta property="og:url" content="https://ml-udm.github.io/" />
<meta property="og:site_name" content="ml-udm.github.io" />
<script type="application/ld+json">
{"@type":"WebSite","url":"https://ml-udm.github.io/","headline":"Workshop on Machine Learning and User Decision Making","name":"ml-udm.github.io","@context":"http://schema.org"}</script>
<!-- End Jekyll SEO tag -->

    <link rel="stylesheet" href="/assets/css/style.css?v=1fd887da03f36fc680648dc1ab81fd840d673d54">
  </head>

<body>

<div class="container-lg px-3 my-5 markdown-body">

<h2>Title:</h2> 
  
On Reinforcement Learning using Monte Carlo Tree Search with Supervised Learning: Non-Asymptotic Analysis</h2>

<h2>Speaker:</h2> 
  
Devavrat Shah, MIT
  
<h2>Abstract:</h2>

<p>
Inspired by the success of AlphaGo Zero (AGZ) which utilizes Monte Carlo Tree Search (MCTS) with Supervised Learning via Neural Network to learn the optimal policy and value function, in this work, we focus on establishing formally that such an approach indeed finds optimal policy asymptotically, as well as establishing non-asymptotic guarantees in the process. We shall focus on infinite-horizon discounted Markov Decision Process to establish the results. To start with, it requires establishing the MCTS’s claimed property in the literature (cf. Kocsis and Szepesvari  (2006 ); Kocsis et al. (2006 )) that for any given query state, MCTS provides approximate value function for the state with enough simulation steps of MDP. We provide non-asymptotic analysis establishing this property by analyzing a non-stationary multi-arm bandit setup. Our proof suggests that MCTS needs to be utilized with polynomial rather than logarithmic “upper confidence bound” for establishing its desired performance – interestingly enough, AGZ chooses such a polynomial bound.
</p>
<p>  
Using this as a building block, combined with nearest neighbor supervised learning, we argue that MCTS acts as a “policy improvement” operator; it has a natural “bootstrapping” property to iteratively improve value function approximation for all  states, due to combining with supervised learning, despite evaluating at only finitely many states. In effect, we establish that to learn approximation of value function (infinity norm) MCTS combined with nearest-neighbors requires nearly optimal number of samples. 
</p>
<p>
This is based on joint work with Qiaomin Xie (Cornell) and Zhi Xu (MIT).
</p>

<p>
Back to the <a href="https://ml-udm.github.io">workshop page</a>  
</p>
  
</div>
  
</body>  
  
</html>
